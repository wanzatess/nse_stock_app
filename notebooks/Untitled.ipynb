{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21167638-1548-41fd-bd59-bfc5ca707219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date', 'Code', 'Name', '12m Low', '12m High', 'Day Low', 'Day High',\n",
      "       'Day Price', 'Previous', 'Change', 'Change%', 'Volume',\n",
      "       'Adjusted Price'],\n",
      "      dtype='object')\n",
      "       Date  Code                     Name 12m Low 12m High Day Low Day High  \\\n",
      "0  2-Jan-24  EGAD              Eaagads Ltd   10.35     14.5    12.8     12.8   \n",
      "1  2-Jan-24  KUKZ               Kakuzi Plc     342      440     385      385   \n",
      "2  2-Jan-24  KAPC  Kapchorua Tea Kenya Plc     207      280     215      215   \n",
      "3  2-Jan-24  LIMT           Limuru Tea Plc     365      380     380      380   \n",
      "4  2-Jan-24  SASN               Sasini Plc    15.1       22      20       20   \n",
      "\n",
      "  Day Price Previous Change Change%    Volume Adjusted Price  \n",
      "0      12.8    13.95  -1.15  -8.24%       100              -  \n",
      "1       385      385      -       -         -              -  \n",
      "2       215      215      -       -         -              -  \n",
      "3       380      380      -       -         -              -  \n",
      "4        20       20      -       -  3,300.00              -  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/NSE_data_all_stocks_2024.csv\")\n",
    "print(df.columns)\n",
    "print(df.iloc[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8958e7bb-7707-4589-ae14-5f36efcd9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSVs: ['NSE_data_all_stocks_2013.csv', 'NSE_data_all_stocks_2014.csv', 'NSE_data_all_stocks_2015.csv', 'NSE_data_all_stocks_2016.csv', 'NSE_data_all_stocks_2017.csv', 'NSE_data_all_stocks_2018.csv', 'NSE_data_all_stocks_2019.csv', 'NSE_data_all_stocks_2020_to_jun30.csv', 'NSE_data_all_stocks_2021_upto_31dec2021.csv', 'NSE_data_all_stocks_2022.csv', 'NSE_data_all_stocks_2023.csv', 'NSE_data_all_stocks_2024.csv', 'NSE_data_all_stocks_2025_jan_to_oct.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_19640\\723393194.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('-', np.nan, inplace=True)\n",
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_19640\\723393194.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('-', np.nan, inplace=True)\n",
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_19640\\723393194.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('-', np.nan, inplace=True)\n",
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_19640\\723393194.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace('-', np.nan, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered data for NSE_20 saved successfully to:\n",
      "../data/processed\\NSE_20_stocks_2013_2025_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# List of companies to keep\n",
    "NSE_20 = [\n",
    "    \"EGAD\", \"KAPC\", \"KUKZ\", \"LIMT\", \"SASN\", \"WTK\", \"CGEN\", \"ABSA\", \"SBIC\",\n",
    "    \"IMH\", \"DTK\", \"SCBK\", \"EQTY\", \"COOP\", \"BKG\", \"HFCK\", \"KCB\", \"NCBA\",\n",
    "    \"XPRS\", \"SMER\", \"KQ\", \"NMG\", \"SGL\", \"TPSE\", \"SCAN\", \"UCHM\", \"LKL\",\n",
    "    \"DCON\", \"NBV\", \"ARM\", \"BAMB\", \"CRWN\", \"CABL\", \"PORT\", \"TOTL\", \"KEGN\",\n",
    "    \"KPLC\", \"UMME\", \"JUB\", \"SLM\", \"KNRE\", \"LBTY\", \"BRIT\", \"CIC\", \"OCH\",\n",
    "    \"CTUM\", \"TCL\", \"HAFR\", \"KUVR\", \"NSE\", \"BOC\", \"BAT\", \"CARB\", \"EABL\",\n",
    "    \"MSC\", \"UNGA\", \"EVRD\", \"AMAC\", \"FTGH\", \"SKL.O0000\", \"SCOM\", \"LAPR\",\n",
    "    \"GLD\", \"SMWF.E0000\"\n",
    "]\n",
    "\n",
    "# Paths\n",
    "data_folder = \"../data/raw\"\n",
    "processed_folder = \"../data/processed\"\n",
    "os.makedirs(processed_folder, exist_ok=True)\n",
    "\n",
    "# List all CSV files\n",
    "csv_files = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "print(\"Found CSVs:\", csv_files)\n",
    "\n",
    "# Columns to convert to numeric\n",
    "numeric_cols = ['12m Low', '12m High', 'Day Low', 'Day High', 'Day Price', 'Previous', 'Change', 'Volume', 'Adjusted Price']\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Standardize column headers\n",
    "    df.columns = [col.strip().title() for col in df.columns]\n",
    "    \n",
    "    # Replace '-' with NaN\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '')\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert Date\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y', errors='coerce')\n",
    "    \n",
    "    # Standardize Code and Name\n",
    "    if 'Code' in df.columns:\n",
    "        df['Code'] = df['Code'].str.upper()\n",
    "    if 'Name' in df.columns:\n",
    "        df['Name'] = df['Name'].str.title()\n",
    "    \n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine all years\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# Fill missing Adjusted Price\n",
    "combined_df['Adjusted Price'] = combined_df['Adjusted Price'].fillna(combined_df['Day Price'])\n",
    "\n",
    "# Calculate missing Change%\n",
    "def calc_change_pct(row):\n",
    "    if pd.notnull(row['Day Price']) and pd.notnull(row['Previous']):\n",
    "        return round((row['Day Price'] - row['Previous']) / row['Previous'] * 100, 2)\n",
    "    return np.nan\n",
    "\n",
    "combined_df['Change%'] = combined_df.apply(\n",
    "    lambda row: calc_change_pct(row) if pd.isna(row['Change%']) else row['Change%'], axis=1\n",
    ")\n",
    "\n",
    "# Filter only NSE_20 companies\n",
    "filtered_df = combined_df[combined_df['Code'].isin(NSE_20)].reset_index(drop=True)\n",
    "\n",
    "# Save filtered dataset\n",
    "output_file = os.path.join(processed_folder, \"NSE_20_stocks_2013_2025_cleaned.csv\")\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Filtered data for NSE_20 saved successfully to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff6f0a4-9f08-4b97-83d0-4c70fa166f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Date'\n",
      "'Code'\n",
      "'Name'\n",
      "'12M Low'\n",
      "'12M High'\n",
      "'Day Low'\n",
      "'Day High'\n",
      "'Day Price'\n",
      "'Previous'\n",
      "'Change'\n",
      "'Change%'\n",
      "'Volume'\n",
      "'Adjust'\n",
      "'Adjusted Price'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df = pd.read_csv(\"../data/processed/NSE_20_stocks_2013_2025_cleaned.csv\")\n",
    "\n",
    "# Show all columns with their repr to reveal hidden spaces\n",
    "for col in filtered_df.columns:\n",
    "    print(repr(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22eaef0f-5667-476e-a396-b9176b7d2a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features created and saved successfully to:\n",
      "../data/processed/NSE_20_stocks_2013_2025_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "filtered_df = pd.read_csv(\"../data/processed/NSE_20_stocks_2013_2025_cleaned.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Clean column names\n",
    "# -------------------------\n",
    "filtered_df.columns = (\n",
    "    filtered_df.columns\n",
    "    .str.strip()\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace('%', 'pct')\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Convert numeric columns\n",
    "# -------------------------\n",
    "numeric_cols = ['12m_low', '12m_high', 'day_low', 'day_high', 'day_price', 'previous', 'change', 'volume', 'adjust', 'adjusted_price']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in filtered_df.columns:\n",
    "        # Remove commas, convert to numeric, coerce errors to NaN\n",
    "        filtered_df[col] = filtered_df[col].astype(str).str.replace(',', '').str.strip()\n",
    "        filtered_df[col] = pd.to_numeric(filtered_df[col], errors='coerce')\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Feature creation\n",
    "# -------------------------\n",
    "\n",
    "# % distance from 12-month Low/High\n",
    "filtered_df['pct_from_12m_low'] = ((filtered_df['day_price'] - filtered_df['12m_low']) / filtered_df['12m_low']) * 100\n",
    "filtered_df['pct_from_12m_high'] = ((filtered_df['12m_high'] - filtered_df['day_price']) / filtered_df['12m_high']) * 100\n",
    "\n",
    "# Daily return\n",
    "filtered_df['daily_return'] = filtered_df['day_price'] - filtered_df['previous']\n",
    "\n",
    "# Daily volatility\n",
    "filtered_df['daily_volatility'] = filtered_df['day_high'] - filtered_df['day_low']\n",
    "\n",
    "# Moving averages (5-day and 10-day) for each stock\n",
    "filtered_df = filtered_df.sort_values(['code', 'date'])\n",
    "filtered_df['ma_5'] = filtered_df.groupby('code')['day_price'].transform(lambda x: x.rolling(5).mean())\n",
    "filtered_df['ma_10'] = filtered_df.groupby('code')['day_price'].transform(lambda x: x.rolling(10).mean())\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Save dataset with features\n",
    "# -------------------------\n",
    "output_file = \"../data/processed/NSE_20_stocks_2013_2025_features.csv\"\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"✅ Features created and saved successfully to:\\n{output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd001b6a-422b-4e81-bba5-8a17db3df405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1.4 complete:\n",
      " - Full dataset with target saved to: ../data/processed/NSE_20_stocks_2013_2025_features_target.csv\n",
      " - Last 30 days filtered dataset saved for dashboard: ../data/processed/NSE_20_stocks_last_30days.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load dataset already processed with features (from 1.3)\n",
    "filtered_df = pd.read_csv(\"../data/processed/NSE_20_stocks_2013_2025_features.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Convert date to datetime\n",
    "# -------------------------\n",
    "filtered_df['date'] = pd.to_datetime(filtered_df['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Create next-day target\n",
    "# -------------------------\n",
    "# Shift day_price by -1 per stock to get next-day price\n",
    "filtered_df['next_day_price'] = filtered_df.groupby('code')['day_price'].shift(-1)\n",
    "\n",
    "# Compute next-day return\n",
    "filtered_df['next_day_return'] = filtered_df['next_day_price'] - filtered_df['day_price']\n",
    "\n",
    "# Define Buy/Sell/Hold function\n",
    "def buy_sell_hold(x, threshold=0.5):\n",
    "    if x > threshold:\n",
    "        return 'Buy'\n",
    "    elif x < -threshold:\n",
    "        return 'Sell'\n",
    "    else:\n",
    "        return 'Hold'\n",
    "\n",
    "# Convert next-day return to % and apply function\n",
    "filtered_df['target'] = (filtered_df['next_day_return'] / filtered_df['day_price'] * 100).apply(buy_sell_hold)\n",
    "\n",
    "# Drop rows where next_day_price is NaN (last day of each stock)\n",
    "filtered_df = filtered_df.dropna(subset=['next_day_price'])\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Filter last 30 days for dashboard\n",
    "# -------------------------\n",
    "end_date = filtered_df['date'].max()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "filtered_last_month = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Save datasets\n",
    "# -------------------------\n",
    "output_file_all = \"../data/processed/NSE_20_stocks_2013_2025_features_target.csv\"\n",
    "output_file_last_month = \"../data/processed/NSE_20_stocks_last_30days.csv\"\n",
    "\n",
    "filtered_df.to_csv(output_file_all, index=False)\n",
    "filtered_last_month.to_csv(output_file_last_month, index=False)\n",
    "\n",
    "print(\"✅ Step 1.4 complete:\")\n",
    "print(f\" - Full dataset with target saved to: {output_file_all}\")\n",
    "print(f\" - Last 30 days filtered dataset saved for dashboard: {output_file_last_month}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ef6933e-b954-4332-82a9-a3ea23f74412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1.5 complete:\n",
      " - Dashboard-ready last 30 days dataset saved: ../data/processed/NSE_20_stocks_last_30days_dashboard.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load dataset with features + target from step 1.4\n",
    "df = pd.read_csv(\"../data/processed/NSE_20_stocks_2013_2025_features_target.csv\")\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Ensure date column is datetime\n",
    "# -------------------------\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Sort by stock code and date\n",
    "# -------------------------\n",
    "df = df.sort_values(['code', 'date']).reset_index(drop=True)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Prepare dataset for frontend/dashboard\n",
    "# -------------------------\n",
    "# Filter last 30 days for plotting in dashboard\n",
    "end_date = df['date'].max()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "df_last_30days = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "# Optional: select only relevant columns for dashboard\n",
    "dashboard_columns = [\n",
    "    'date', 'code', 'name', 'day_price', 'ma_5', 'ma_10',\n",
    "    'pct_from_12m_low', 'pct_from_12m_high', 'daily_return',\n",
    "    'daily_volatility', 'target'\n",
    "]\n",
    "df_last_30days_dashboard = df_last_30days[dashboard_columns]\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Save the dashboard-ready dataset\n",
    "# -------------------------\n",
    "output_file_dashboard = \"../data/processed/NSE_20_stocks_last_30days_dashboard.csv\"\n",
    "df_last_30days_dashboard.to_csv(output_file_dashboard, index=False)\n",
    "\n",
    "print(\"✅ Step 1.5 complete:\")\n",
    "print(f\" - Dashboard-ready last 30 days dataset saved: {output_file_dashboard}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc115aa-8096-4944-a456-1d070e2aa7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
